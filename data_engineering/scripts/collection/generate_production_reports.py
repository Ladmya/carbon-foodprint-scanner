"""
scripts/collection/generate_production_reports.py
PRODUCTION REPORT GENERATOR: Analyze pipeline JSON outputs and generate consolidated reports

This script reads JSON files generated by the extraction/transformation pipeline
and creates comprehensive production-ready reports with actionable insights.
"""

import json
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any
from collections import Counter, defaultdict

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).resolve().parents[2] / "src"))


class ProductionReportGenerator:
    """
    Generate comprehensive production reports from pipeline JSON outputs
    
    CAPABILITIES:
    1. Auto-detect latest pipeline outputs
    2. Cross-correlate extraction, transformation, and analysis data
    3. Calculate production readiness metrics
    4. Generate actionable recommendations
    5. Export consolidated reports
    """
    
    def __init__(self, data_directory: Path = None):
        if data_directory is None:
            # Default to data_engineering/data/ structure - fix path to avoid duplicate data_engineering folder
            self.data_dir = Path(__file__).resolve().parents[3] / "data_engineering" / "data"
        else:
            self.data_dir = Path(data_directory)
        
        self.report_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Data containers
        self.extraction_data = {}
        self.transformation_data = {}
        self.validated_data = {}
        self.rejected_data = {}
        self.analysis_data = {}
        
        # Consolidated metrics
        self.production_metrics = {}
        self.data_quality_summary = {}
        self.business_insights = {}
    
    def generate_comprehensive_report(self, auto_detect: bool = True) -> Dict[str, Any]:
        """
        Generate comprehensive production report
        
        Args:
            auto_detect: If True, automatically find latest JSON files
        """
        print("üìä PRODUCTION REPORT GENERATOR")
        print("=" * 60)
        print(f"üîç Scanning data directory: {self.data_dir}")
        print(f"üìÖ Report timestamp: {self.report_timestamp}")
        print("=" * 60)
        
        try:
            # Step 1: Load pipeline data
            if auto_detect:
                self._auto_detect_and_load_data()
            else:
                self._load_specified_data()
            
            # Step 2: Cross-correlate data
            print(f"\nüîó Cross-correlating pipeline data...")
            self._correlate_pipeline_data()
            
            # Step 3: Calculate production metrics
            print(f"üìä Calculating production metrics...")
            self._calculate_production_metrics()
            
            # Step 4: Analyze data quality
            print(f"üîç Analyzing data quality...")
            self._analyze_data_quality()
            
            # Step 5: Generate business insights
            print(f"üí° Generating business insights...")
            self._generate_business_insights()
            
            # Step 6: Create consolidated report
            print(f"üìã Creating consolidated report...")
            comprehensive_report = self._create_consolidated_report()
            
            # Step 7: Save reports
            print(f"üíæ Saving production reports...")
            saved_files = self._save_reports(comprehensive_report)
            
            print(f"\n‚úÖ PRODUCTION REPORT COMPLETED")
            print(f"üìÅ Reports saved: {len(saved_files)} files")
            for file_path in saved_files:
                print(f"   ‚Üí {file_path.name}")
            
            return comprehensive_report
            
        except Exception as e:
            print(f"‚ùå Report generation failed: {e}")
            import traceback
            traceback.print_exc()
            return {"success": False, "error": str(e)}
    
    def _auto_detect_and_load_data(self):
        """Auto-detect and load latest pipeline JSON files"""
        
        # Define search patterns for different data types
        search_patterns = {
            "extraction": [
                "raw/openfoodfacts/chocolate_extraction_raw_*.json",
                "analysis/extraction_phase/extraction_*.json"
            ],
            "validated": [
                "processed/validated/validated_products_*.json"
            ],
            "transformation": [
                "analysis/transformation_phase/executive_summary_*.json",
                "analysis/transformation_phase/production_readiness_*.json"
            ],
            "rejected": [
                "processed/rejected/rejected_products_*.json"
            ],
            "analysis": [
                "analysis/extraction_phase/extraction_analysis_comprehensive_*.json",
                "analysis/transformation_phase/data_quality_*.json"
            ]
        }
        
        latest_files = {}
        
        for data_type, patterns in search_patterns.items():
            print(f"   üîç Detecting {data_type} files...")
            
            found_files = []
            for pattern in patterns:
                search_path = self.data_dir / pattern.replace("*", "")
                parent_dir = search_path.parent
                file_pattern = search_path.name.replace(".json", "*.json")
                
                if parent_dir.exists():
                    matches = list(parent_dir.glob(file_pattern))
                    found_files.extend(matches)
            
            if found_files:
                # Sort by modification time and take the latest
                latest_file = sorted(found_files, key=lambda f: f.stat().st_mtime)[-1]
                latest_files[data_type] = latest_file
                print(f"      ‚úÖ Latest {data_type}: {latest_file.name}")
            else:
                print(f"      ‚ö†Ô∏è No {data_type} files found")
        
        # Load the detected files
        for data_type, file_path in latest_files.items():
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                if data_type == "extraction":
                    self.extraction_data = data
                elif data_type == "validated":
                    self.validated_data = data
                elif data_type == "transformation":
                    self.transformation_data = data
                elif data_type == "rejected":
                    self.rejected_data = data
                elif data_type == "analysis":
                    self.analysis_data = data
                
                print(f"      üìÑ Loaded {data_type}: {len(str(data))} chars")
                
            except Exception as e:
                print(f"      ‚ùå Failed to load {file_path}: {e}")
    
    def _correlate_pipeline_data(self):
        """Cross-correlate data from different pipeline stages"""
        
        # Extract key datasets
        extracted_products = {}
        validated_products = {}
        rejected_products = {}
        
        # From extraction data
        if "extracted_products" in self.extraction_data:
            extracted_products = self.extraction_data["extracted_products"]
        elif "discovery_results" in self.extraction_data:
            # Handle raw extraction format
            raw_data = self.extraction_data.get("discovery_results", {})
            extracted_products = raw_data.get("discovered_products", {})
        
        # From validated data
        if "validated_products" in self.validated_data:
            validated_products = self.validated_data["validated_products"]
        elif self.validated_data:  # If validated_data is not empty, it might contain the validated products directly
            validated_products = self.validated_data
        
        # From transformation data (fallback)
        if not validated_products and "validated_products" in self.transformation_data:
            validated_products = self.transformation_data["validated_products"]
        
        # From rejected data
        if "rejected_products" in self.rejected_data:
            rejected_products = self.rejected_data["rejected_products"]
        elif self.rejected_data:  # If rejected_data is not empty, it might contain the rejected products directly
            rejected_products = self.rejected_data
        
        print(f"   ‚Üí Extracted products: {len(extracted_products)}")
        print(f"   ‚Üí Validated products: {len(validated_products)}")
        print(f"   ‚Üí Rejected products: {len(rejected_products)}")
        
        # Store correlated data
        self.correlated_data = {
            "extracted_products": extracted_products,
            "validated_products": validated_products,
            "rejected_products": rejected_products,
            "total_pipeline_products": len(extracted_products)
        }
    
    def _calculate_production_metrics(self):
        """Calculate key production readiness metrics"""
        
        extracted = self.correlated_data["extracted_products"]
        validated = self.correlated_data["validated_products"]
        rejected = self.correlated_data["rejected_products"]
        
        total_products = len(extracted)
        
        # Core metrics
        self.production_metrics = {
            "pipeline_overview": {
                "total_products_processed": total_products,
                "products_validated": len(validated),
                "products_rejected": len(rejected),
                "validation_success_rate": (len(validated) / total_products * 100) if total_products > 0 else 0,
                "rejection_rate": (len(rejected) / total_products * 100) if total_products > 0 else 0
            }
        }
        
        # Detailed field analysis
        field_completeness = self._analyze_field_completeness(validated)
        self.production_metrics["field_completeness"] = field_completeness
        
        # Database readiness analysis
        db_ready_products = self._count_database_ready_products(validated)
        self.production_metrics["database_readiness"] = db_ready_products
        
        # Bot functionality analysis
        bot_ready_products = self._count_bot_ready_products(validated)
        self.production_metrics["bot_readiness"] = bot_ready_products
        
        print(f"   ‚Üí Validation success rate: {self.production_metrics['pipeline_overview']['validation_success_rate']:.1f}%")
        print(f"   ‚Üí Database ready products: {db_ready_products['total_ready']}")
        print(f"   ‚Üí Bot ready products: {bot_ready_products['total_ready']}")
    
    def _analyze_field_completeness(self, validated_products: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze completeness of each field across validated products"""
        
        field_stats = {
            "barcode": {"present": 0, "missing": 0},
            "product_name": {"present": 0, "missing": 0},
            "brand_name": {"present": 0, "missing": 0},
            "weight": {"present": 0, "missing": 0},
            "co2_total": {"present": 0, "missing": 0},
            "nutriscore_grade": {"present": 0, "missing": 0},
            "nutriscore_score": {"present": 0, "missing": 0},
            "transport_equivalents": {"present": 0, "missing": 0}
        }
        
        for barcode, product_info in validated_products.items():
            # Handle different data structures
            if "transformed_data" in product_info:
                product_data = product_info["transformed_data"]
            else:
                product_data = product_info
            
            # Check each field
            for field in field_stats.keys():
                if field == "transport_equivalents":
                    # Check if transport equivalents were calculated
                    has_transport = any([
                        product_data.get("co2_vehicle_km") is not None,
                        product_data.get("co2_train_km") is not None,
                        product_data.get("co2_bus_km") is not None,
                        product_data.get("co2_plane_km") is not None
                    ])
                    if has_transport:
                        field_stats[field]["present"] += 1
                    else:
                        field_stats[field]["missing"] += 1
                else:
                    value = product_data.get(field)
                    if value is not None and value != "":
                        field_stats[field]["present"] += 1
                    else:
                        field_stats[field]["missing"] += 1
        
        # Calculate percentages
        total_products = len(validated_products)
        for field, stats in field_stats.items():
            stats["completion_rate"] = (stats["present"] / total_products * 100) if total_products > 0 else 0
            stats["quality_grade"] = self._get_quality_grade(stats["completion_rate"])
        
        return field_stats
    
    def _count_database_ready_products(self, validated_products: Dict[str, Any]) -> Dict[str, Any]:
        """Count products ready for database insertion"""
        
        db_ready_count = 0
        missing_required_fields = Counter()
        
        required_fields = ["barcode", "product_name", "brand_name"]
        
        for barcode, product_info in validated_products.items():
            if "transformed_data" in product_info:
                product_data = product_info["transformed_data"]
            else:
                product_data = product_info
            
            missing_fields = []
            for field in required_fields:
                value = product_data.get(field)
                if not value or value == "":
                    missing_fields.append(field)
            
            if not missing_fields:
                db_ready_count += 1
            else:
                for field in missing_fields:
                    missing_required_fields[field] += 1
        
        return {
            "total_ready": db_ready_count,
            "total_validated": len(validated_products),
            "ready_percentage": (db_ready_count / len(validated_products) * 100) if validated_products else 0,
            "missing_required_fields": dict(missing_required_fields),
            "readiness_grade": self._get_quality_grade((db_ready_count / len(validated_products) * 100) if validated_products else 0)
        }
    
    def _count_bot_ready_products(self, validated_products: Dict[str, Any]) -> Dict[str, Any]:
        """Count products ready for Telegram bot functionality"""
        
        bot_ready_count = 0
        functionality_issues = Counter()
        
        # Bot requires: barcode, product_name, brand_name, co2_total, nutriscore
        for barcode, product_info in validated_products.items():
            if "transformed_data" in product_info:
                product_data = product_info["transformed_data"]
            else:
                product_data = product_info
            
            issues = []
            
            # Check critical bot fields
            if not product_data.get("barcode"):
                issues.append("missing_barcode")
            if not product_data.get("product_name"):
                issues.append("missing_product_name")
            if not product_data.get("brand_name"):
                issues.append("missing_brand_name")
            if not product_data.get("co2_total"):
                issues.append("missing_co2_data")
            
            # Check nutriscore (need at least one)
            has_nutriscore = (
                product_data.get("nutriscore_grade") or 
                product_data.get("nutriscore_score") is not None
            )
            if not has_nutriscore:
                issues.append("missing_nutriscore")
            
            # Check transport equivalents
            has_transport = any([
                product_data.get("co2_vehicle_km") is not None,
                product_data.get("co2_train_km") is not None
            ])
            if not has_transport:
                issues.append("missing_transport_equivalents")
            
            if not issues:
                bot_ready_count += 1
            else:
                for issue in issues:
                    functionality_issues[issue] += 1
        
        return {
            "total_ready": bot_ready_count,
            "total_validated": len(validated_products),
            "ready_percentage": (bot_ready_count / len(validated_products) * 100) if validated_products else 0,
            "functionality_issues": dict(functionality_issues.most_common()),
            "bot_launch_feasible": bot_ready_count >= 100,  # Minimum for launch
            "readiness_grade": self._get_quality_grade((bot_ready_count / len(validated_products) * 100) if validated_products else 0)
        }
    
    def _analyze_data_quality(self):
        """Analyze overall data quality across the pipeline"""
        
        validated = self.correlated_data["validated_products"]
        rejected = self.correlated_data["rejected_products"]
        
        # Quality by brand analysis
        brand_quality = self._analyze_quality_by_brand(validated)
        
        # CO2 data quality analysis
        co2_quality = self._analyze_co2_data_quality(validated)
        
        # Weight data quality analysis
        weight_quality = self._analyze_weight_data_quality(validated)
        
        # Rejection reasons analysis
        rejection_analysis = self._analyze_rejection_reasons(rejected)
        
        self.data_quality_summary = {
            "overall_quality_score": self._calculate_overall_quality_score(),
            "brand_quality_analysis": brand_quality,
            "co2_data_quality": co2_quality,
            "weight_data_quality": weight_quality,
            "rejection_analysis": rejection_analysis
        }
        
        print(f"   ‚Üí Overall quality score: {self.data_quality_summary['overall_quality_score']:.1f}%")
        print(f"   ‚Üí Brands analyzed: {len(brand_quality)}")
        print(f"   ‚Üí CO2 data quality: {co2_quality['quality_grade']}")
    
    def _analyze_quality_by_brand(self, validated_products: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze data quality by brand"""
        
        brand_stats = defaultdict(lambda: {
            "total_products": 0,
            "complete_products": 0,
            "co2_coverage": 0,
            "weight_coverage": 0,
            "nutriscore_coverage": 0
        })
        
        for barcode, product_info in validated_products.items():
            if "transformed_data" in product_info:
                product_data = product_info["transformed_data"]
            else:
                product_data = product_info
            
            brand = product_data.get("brand_name", "Unknown")
            brand_stats[brand]["total_products"] += 1
            
            # Check completeness
            has_co2 = product_data.get("co2_total") is not None
            has_weight = product_data.get("weight") is not None
            has_nutriscore = (
                product_data.get("nutriscore_grade") or 
                product_data.get("nutriscore_score") is not None
            )
            
            if has_co2:
                brand_stats[brand]["co2_coverage"] += 1
            if has_weight:
                brand_stats[brand]["weight_coverage"] += 1
            if has_nutriscore:
                brand_stats[brand]["nutriscore_coverage"] += 1
            
            # Complete product (has all key fields)
            if has_co2 and has_weight and has_nutriscore:
                brand_stats[brand]["complete_products"] += 1
        
        # Calculate percentages and grades
        for brand, stats in brand_stats.items():
            total = stats["total_products"]
            if total > 0:
                stats["completeness_rate"] = (stats["complete_products"] / total * 100)
                stats["co2_rate"] = (stats["co2_coverage"] / total * 100)
                stats["weight_rate"] = (stats["weight_coverage"] / total * 100)
                stats["nutriscore_rate"] = (stats["nutriscore_coverage"] / total * 100)
                stats["quality_grade"] = self._get_quality_grade(stats["completeness_rate"])
        
        return dict(brand_stats)
    
    def _analyze_co2_data_quality(self, validated_products: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze CO2 data quality and distribution"""
        
        co2_values = []
        co2_sources = Counter()
        impact_levels = Counter()
        
        for barcode, product_info in validated_products.items():
            if "transformed_data" in product_info:
                product_data = product_info["transformed_data"]
            else:
                product_data = product_info
            
            co2_value = product_data.get("co2_total")
            if co2_value is not None:
                co2_values.append(co2_value)
                
                # Track impact level
                impact_level = product_data.get("impact_level", "Unknown")
                impact_levels[impact_level] += 1
        
        if co2_values:
            avg_co2 = sum(co2_values) / len(co2_values)
            min_co2 = min(co2_values)
            max_co2 = max(co2_values)
            
            quality_grade = self._get_quality_grade((len(co2_values) / len(validated_products) * 100))
        else:
            avg_co2 = min_co2 = max_co2 = 0
            quality_grade = "F"
        
        return {
            "total_products_with_co2": len(co2_values),
            "total_products": len(validated_products),
            "coverage_rate": (len(co2_values) / len(validated_products) * 100) if validated_products else 0,
            "average_co2": round(avg_co2, 1),
            "min_co2": min_co2,
            "max_co2": max_co2,
            "impact_level_distribution": dict(impact_levels),
            "quality_grade": quality_grade
        }
    
    def _analyze_weight_data_quality(self, validated_products: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze weight data quality"""
        
        weights = []
        units = Counter()
        
        for barcode, product_info in validated_products.items():
            if "transformed_data" in product_info:
                product_data = product_info["transformed_data"]
            else:
                product_data = product_info
            
            weight = product_data.get("weight")
            unit = product_data.get("product_quantity_unit", "unknown")
            
            if weight is not None:
                weights.append(weight)
                units[unit] += 1
        
        return {
            "total_products_with_weight": len(weights),
            "coverage_rate": (len(weights) / len(validated_products) * 100) if validated_products else 0,
            "average_weight": round(sum(weights) / len(weights), 1) if weights else 0,
            "unit_distribution": dict(units),
            "quality_grade": self._get_quality_grade((len(weights) / len(validated_products) * 100) if validated_products else 0)
        }
    
    def _analyze_rejection_reasons(self, rejected_products: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze why products were rejected"""
        
        rejection_reasons = Counter()
        
        for barcode, rejection_info in rejected_products.items():
            reasons = rejection_info.get("rejection_reasons", [])
            for reason in reasons:
                rejection_reasons[reason] += 1
        
        return {
            "total_rejected": len(rejected_products),
            "top_rejection_reasons": dict(rejection_reasons.most_common(10)),
            "rejection_categories": self._categorize_rejection_reasons(rejection_reasons)
        }
    
    def _categorize_rejection_reasons(self, rejection_reasons: Counter) -> Dict[str, int]:
        """Categorize rejection reasons into broader categories"""
        
        categories = {
            "missing_critical_fields": 0,
            "missing_co2_data": 0,
            "missing_product_info": 0,
            "data_quality_issues": 0
        }
        
        for reason, count in rejection_reasons.items():
            reason_lower = reason.lower()
            if "barcode" in reason_lower or "product_name" in reason_lower or "brand" in reason_lower:
                categories["missing_critical_fields"] += count
            elif "co2" in reason_lower:
                categories["missing_co2_data"] += count
            elif "nutriscore" in reason_lower:
                categories["missing_product_info"] += count
            else:
                categories["data_quality_issues"] += count
        
        return categories
    
    def _calculate_overall_quality_score(self) -> float:
        """Calculate overall data quality score"""
        
        # Weight different metrics
        metrics = self.production_metrics
        
        if not metrics:
            return 0.0
        
        validation_rate = metrics["pipeline_overview"]["validation_success_rate"]
        db_readiness = metrics["database_readiness"]["ready_percentage"]
        bot_readiness = metrics["bot_readiness"]["ready_percentage"]
        
        # Weighted average (bot readiness is most important)
        overall_score = (
            validation_rate * 0.3 +
            db_readiness * 0.3 +
            bot_readiness * 0.4
        )
        
        return round(overall_score, 1)
    
    def _generate_business_insights(self):
        """Generate actionable business insights"""
        
        metrics = self.production_metrics
        quality = self.data_quality_summary
        
        # Production readiness assessment
        bot_ready = metrics["bot_readiness"]["total_ready"]
        bot_percentage = metrics["bot_readiness"]["ready_percentage"]
        
        # Business recommendations
        recommendations = []
        priorities = []
        
        if bot_ready >= 200:
            recommendations.append("‚úÖ READY FOR FULL PRODUCTION LAUNCH")
            recommendations.append("Scale data collection to 1000+ products")
            priorities.append("HIGH: Implement automated monitoring")
        elif bot_ready >= 100:
            recommendations.append("‚ö° READY FOR LIMITED BETA LAUNCH")
            recommendations.append("Test with current dataset while improving coverage")
            priorities.append("MEDIUM: Expand to more brands/categories")
        elif bot_ready >= 50:
            recommendations.append("üß™ READY FOR INTERNAL TESTING")
            recommendations.append("Fix critical issues before beta launch")
            priorities.append("HIGH: Address top rejection reasons")
        else:
            recommendations.append("‚ùå NOT READY FOR LAUNCH")
            recommendations.append("Major improvements needed in data pipeline")
            priorities.append("URGENT: Fix extraction/validation issues")
        
        # Specific improvement areas
        if bot_percentage < 80:
            issues = metrics["bot_readiness"]["functionality_issues"]
            top_issue = max(issues.items(), key=lambda x: x[1]) if issues else None
            if top_issue:
                priorities.append(f"TOP PRIORITY: Fix {top_issue[0]} ({top_issue[1]} products affected)")
        
        self.business_insights = {
            "launch_readiness": {
                "status": "PRODUCTION_READY" if bot_ready >= 200 else "BETA_READY" if bot_ready >= 100 else "TESTING_READY" if bot_ready >= 50 else "NOT_READY",
                "bot_ready_products": bot_ready,
                "estimated_user_queries_supported": bot_ready * 0.8,  # Assuming 80% query success rate
                "launch_timeline": "Immediate" if bot_ready >= 200 else "2-4 weeks" if bot_ready >= 100 else "4-8 weeks" if bot_ready >= 50 else "8+ weeks"
            },
            "recommendations": recommendations,
            "priority_actions": priorities,
            "revenue_potential": self._estimate_revenue_potential(bot_ready),
            "risk_assessment": self._assess_risks()
        }
    
    def _estimate_revenue_potential(self, bot_ready_products: int) -> Dict[str, Any]:
        """Estimate revenue potential based on ready products"""
        
        # Simple revenue estimation model
        estimated_daily_users = min(bot_ready_products * 2, 1000)  # Cap at 1000 daily users
        estimated_queries_per_user = 3
        estimated_conversion_rate = 0.02  # 2% of users might pay for premium features
        estimated_monthly_revenue_per_user = 5  # 5‚Ç¨ per month
        
        monthly_revenue = estimated_daily_users * estimated_conversion_rate * estimated_monthly_revenue_per_user * 30
        
        return {
            "estimated_daily_users": estimated_daily_users,
            "estimated_monthly_revenue_eur": round(monthly_revenue, 2),
            "estimated_yearly_revenue_eur": round(monthly_revenue * 12, 2),
            "product_coverage_impact": f"Currently supports {bot_ready_products} products"
        }
    
    def _assess_risks(self) -> List[str]:
        """Assess business and technical risks"""
        
        risks = []
        
        metrics = self.production_metrics
        quality = self.data_quality_summary
        
        # Data quality risks
        if quality["overall_quality_score"] < 75:
            risks.append("HIGH: Low data quality may lead to user dissatisfaction")
        
        # Coverage risks
        bot_ready = metrics["bot_readiness"]["total_ready"]
        if bot_ready < 100:
            risks.append("HIGH: Limited product coverage may frustrate users")
        
        # CO2 data risks
        co2_coverage = quality["co2_data_quality"]["coverage_rate"]
        if co2_coverage < 80:
            risks.append("MEDIUM: Incomplete CO2 data affects core functionality")
        
        # Technical risks
        rejection_rate = metrics["pipeline_overview"]["rejection_rate"]
        if rejection_rate > 30:
            risks.append("MEDIUM: High rejection rate indicates pipeline instability")
        
        return risks
    
    def _create_consolidated_report(self) -> Dict[str, Any]:
        """Create comprehensive consolidated report"""
        
        return {
            "report_metadata": {
                "generation_timestamp": datetime.now().isoformat(),
                "report_type": "production_readiness_assessment",
                "data_sources": {
                    "extraction_data_loaded": bool(self.extraction_data),
                    "transformation_data_loaded": bool(self.transformation_data),
                    "analysis_data_loaded": bool(self.analysis_data)
                }
            },
            "executive_summary": {
                "total_products_processed": self.correlated_data["total_pipeline_products"],
                "products_ready_for_bot": self.production_metrics["bot_readiness"]["total_ready"],
                "overall_quality_score": self.data_quality_summary["overall_quality_score"],
                "launch_readiness_status": self.business_insights["launch_readiness"]["status"],
                "estimated_timeline": self.business_insights["launch_readiness"]["launch_timeline"],
                "key_recommendation": self.business_insights["recommendations"][0] if self.business_insights["recommendations"] else "No recommendations available"
            },
            "detailed_metrics": self.production_metrics,
            "data_quality_analysis": self.data_quality_summary,
            "business_insights": self.business_insights,
            "correlated_data_summary": {
                key: len(value) if isinstance(value, dict) else value 
                for key, value in self.correlated_data.items()
            }
        }
    
    def _save_reports(self, comprehensive_report: Dict[str, Any]) -> List[Path]:
        """Save reports in multiple formats"""
        
        output_dir = self.data_dir / "reports" / "production" / "production_readiness"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        saved_files = []
        
        # 1. Full comprehensive report (JSON)
        full_report_file = output_dir / f"comprehensive_production_report_{self.report_timestamp}.json"
        with open(full_report_file, 'w', encoding='utf-8') as f:
            json.dump(comprehensive_report, f, indent=2, ensure_ascii=False, default=str)
        saved_files.append(full_report_file)
        
        # 2. Executive summary (JSON)
        exec_summary_file = self.data_dir / "reports" / "production" / "executive_summary" / f"executive_summary_{self.report_timestamp}.json"
        exec_summary = {
            "executive_summary": comprehensive_report["executive_summary"],
            "business_insights": comprehensive_report["business_insights"],
            "top_recommendations": comprehensive_report["business_insights"]["recommendations"][:3]
        }
        with open(exec_summary_file, 'w', encoding='utf-8') as f:
            json.dump(exec_summary, f, indent=2, ensure_ascii=False, default=str)
        saved_files.append(exec_summary_file)
        
        # 3. Human-readable summary (TXT)
        readable_report_file = self.data_dir / "reports" / "production" / "executive_summary" / f"production_summary_{self.report_timestamp}.txt"
        with open(readable_report_file, 'w', encoding='utf-8') as f:
            f.write(self._generate_readable_summary(comprehensive_report))
        saved_files.append(readable_report_file)
        
        return saved_files
    
    def _generate_readable_summary(self, report: Dict[str, Any]) -> str:
        """Generate human-readable summary"""
        
        exec_summary = report["executive_summary"]
        business = report["business_insights"]
        metrics = report["detailed_metrics"]
        quality = report["data_quality_analysis"]
        
        summary = f"""
üç´ CARBON FOODPRINT SCANNER - PRODUCTION READINESS REPORT
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
============================================================

üìä EXECUTIVE SUMMARY
   ‚Üí Total Products Processed: {exec_summary['total_products_processed']}
   ‚Üí Products Ready for Bot: {exec_summary['products_ready_for_bot']}
   ‚Üí Overall Quality Score: {exec_summary['overall_quality_score']:.1f}%
   ‚Üí Launch Status: {exec_summary['launch_readiness_status']}
   ‚Üí Estimated Timeline: {exec_summary['estimated_timeline']}

üéØ KEY METRICS
   ‚Üí Validation Success Rate: {metrics['pipeline_overview']['validation_success_rate']:.1f}%
   ‚Üí Database Ready Products: {metrics['database_readiness']['total_ready']} ({metrics['database_readiness']['ready_percentage']:.1f}%)
   ‚Üí Bot Ready Products: {metrics['bot_readiness']['total_ready']} ({metrics['bot_readiness']['ready_percentage']:.1f}%)
   ‚Üí Rejection Rate: {metrics['pipeline_overview']['rejection_rate']:.1f}%

üîç DATA QUALITY BY FIELD
"""
        
        # Add field completeness
        for field, stats in metrics["field_completeness"].items():
            summary += f"   ‚Üí {field}: {stats['completion_rate']:.1f}% (Grade: {stats['quality_grade']})\n"
        
        summary += f"""
üåç CO2 DATA QUALITY
   ‚Üí Coverage: {quality['co2_data_quality']['coverage_rate']:.1f}%
   ‚Üí Average CO2: {quality['co2_data_quality']['average_co2']} g/100g
   ‚Üí Quality Grade: {quality['co2_data_quality']['quality_grade']}

üè∑Ô∏è TOP BRANDS BY QUALITY
"""
        
        # Add top brands
        brand_quality = quality["brand_quality_analysis"]
        sorted_brands = sorted(brand_quality.items(), key=lambda x: x[1]["completeness_rate"], reverse=True)
        for brand, stats in sorted_brands[:5]:
            summary += f"   ‚Üí {brand}: {stats['completeness_rate']:.1f}% complete ({stats['total_products']} products)\n"
        
        summary += f"""
üí∞ BUSINESS INSIGHTS
   ‚Üí Launch Readiness: {business['launch_readiness']['status']}
   ‚Üí Estimated Daily Users: {business['revenue_potential']['estimated_daily_users']}
   ‚Üí Estimated Monthly Revenue: ‚Ç¨{business['revenue_potential']['estimated_monthly_revenue_eur']}
   ‚Üí Timeline to Launch: {business['launch_readiness']['launch_timeline']}

üìã TOP RECOMMENDATIONS
"""
        for i, rec in enumerate(business["recommendations"][:3], 1):
            summary += f"   {i}. {rec}\n"
        
        summary += f"""
üî• PRIORITY ACTIONS
"""
        for i, action in enumerate(business["priority_actions"][:3], 1):
            summary += f"   {i}. {action}\n"
        
        if business["risk_assessment"]:
            summary += f"""
‚ö†Ô∏è RISK ASSESSMENT
"""
            for i, risk in enumerate(business["risk_assessment"], 1):
                summary += f"   {i}. {risk}\n"
        
        summary += f"""
============================================================
Report generated by Carbon Foodprint Scanner Production Pipeline
Data Engineering Team - {datetime.now().year}
"""
        
        return summary
    
    def _get_quality_grade(self, percentage: float) -> str:
        """Convert percentage to quality grade"""
        if percentage >= 95:
            return "A"
        elif percentage >= 85:
            return "B"
        elif percentage >= 75:
            return "C"
        elif percentage >= 65:
            return "D"
        else:
            return "F"


def main():
    """Main execution function"""
    
    print("üöÄ PRODUCTION REPORT GENERATOR")
    print("Analyzing pipeline outputs and generating comprehensive reports...")
    print("=" * 60)
    
    # Initialize generator
    generator = ProductionReportGenerator()
    
    # Generate comprehensive report
    report = generator.generate_comprehensive_report(auto_detect=True)
    
    if report.get("success", True):  # Assume success if not explicitly failed
        print(f"\nüéâ PRODUCTION REPORT GENERATION COMPLETED!")
        
        exec_summary = report.get("executive_summary", {})
        if exec_summary:
            print(f"\nüìä QUICK SUMMARY:")
            print(f"   ‚Üí Products ready for bot: {exec_summary.get('products_ready_for_bot', 'N/A')}")
            print(f"   ‚Üí Overall quality: {exec_summary.get('overall_quality_score', 'N/A')}%")
            print(f"   ‚Üí Launch status: {exec_summary.get('launch_readiness_status', 'N/A')}")
            print(f"   ‚Üí Timeline: {exec_summary.get('estimated_timeline', 'N/A')}")
        
        return 0
    else:
        print(f"\n‚ùå REPORT GENERATION FAILED!")
        print(f"Error: {report.get('error', 'Unknown error')}")
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)